{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with langchain and Open AI\n",
    "\n",
    "Outline:\n",
    "- Setup Langchain, langsmith and Langserve✅\n",
    "- Use the basic and common component of LagChain, primpt template, models and output parser✅\n",
    "- Buils a simple application with langchain✅\n",
    "- Trace the application with langsmith✅\n",
    "- serve the application with langserve✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For langchain to work, you need to have the following environment variables set:\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001E1F1F7EF80>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001E1F1FA5570>, root_client=<openai.OpenAI object at 0x000001E1F1F7FCD0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E1F1F7E590>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output parser\n",
    "result = llm.invoke(\"Who is the president of Nigeria?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of October 2023, the president of Nigeria is Bola Ahmed Tinubu, who has been in office since May 29, 2023.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 14, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None} id='run-2e35d1b6-08c4-472a-ac0e-3250a4b1544f-0' usage_metadata={'input_tokens': 14, 'output_tokens': 32, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = llm.invoke(\"What is mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mars is the fourth planet from the Sun in our solar system. It is often referred to as the \"Red Planet\" due to its reddish appearance, which comes from iron oxide (rust) on its surface. Here are some key points about Mars:\\n\\n1. **Size and Distance:** Mars is about half the size of Earth, with a diameter of approximately 6,792 kilometers (about 4,220 miles). It is located about 227.9 million kilometers (about 141.6 million miles) from the Sun.\\n\\n2. **Surface and Atmosphere:** Mars has a varied surface with features such as volcanoes, valleys, deserts, and polar ice caps. Olympus Mons, the tallest volcano in the solar system, and Valles Marineris, one of the largest canyons, are located on Mars. The thin atmosphere is composed mostly of carbon dioxide, with trace amounts of nitrogen and argon.\\n\\n3. **Moons:** Mars has two small moons, Phobos and Deimos, which are thought to be captured asteroids.\\n\\n4. **Temperature:** The surface temperature on Mars can vary significantly, ranging from about -125 degrees Celsius (-195 degrees Fahrenheit) during the winter at the poles to 20 degrees Celsius (68 degrees Fahrenheit) in the summer at its equator.\\n\\n5. **Exploration:** Mars has been a focal point for exploration due to the possibility of past life. Numerous missions by NASA and other space agencies have been sent to study Mars, including orbiters, landers, and rovers. Notable missions include the Mars rovers Spirit, Opportunity, Curiosity, and Perseverance.\\n\\n6. **Potential for Life:** While no definitive evidence of life on Mars has been found, scientists continue to study its surface and climate for signs of past microbial life. The presence of water ice and the detection of seasonal methane fluctuations are of particular interest.\\n\\n7. **Human Exploration:** Mars is considered a prime candidate for future human exploration due to its similarities to Earth. Efforts are ongoing to develop the technology needed for human missions to Mars, with plans for eventual manned missions.\\n\\nMars continues to be a subject of interest in astronomy and planetary science due to its unique characteristics and potential for human habitation in the distant future.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 457, 'prompt_tokens': 10, 'total_tokens': 467, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None}, id='run-19db8eae-e266-4b09-b9b4-1d79fe0fbc67-0', usage_metadata={'input_tokens': 10, 'output_tokens': 457, 'total_tokens': 467, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Act like an expert AI Engineer, Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Act like an expert AI Engineer, Provide me answers based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of my last update in October 2023, there isn\\'t any widely recognized technologies or frameworks explicitly named \"Langsmith\" and \"Langserve\" in the context of artificial intelligence or software engineering. It\\'s possible that these could be internal tools, newly released products, or lesser-known initiatives. If they have been introduced or gained popularity after my last update, I would recommend checking official websites, recent technology news articles, or relevant technology forums for the most up-to-date information. Additionally, understanding the context or the domain (e.g., natural language processing, software development tools, etc.) in which these terms are used could provide further clues.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 34, 'total_tokens': 163, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-e7b05b6e-1204-44c7-9516-5e481d2f8a15-0' usage_metadata={'input_tokens': 34, 'output_tokens': 129, 'total_tokens': 163, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## Chain prompt with LLM\n",
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Tell me about langsmith and langserve.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative Adversarial Networks (GANs) are a class of machine learning frameworks invented by Ian Goodfellow and his colleagues in 2014. GANs are particularly known for their capability to generate new, synthetic instances of data that can pass for real data. They are widely used in image generation, video generation, and voice synthesis, among other applications.\\n\\nA GAN consists of two primary components: the generator and the discriminator, which are neural networks that contest with each other in a zero-sum game framework.\\n\\n1. **Generator:** This network is responsible for creating new data instances. It starts from a random noise and tries to generate data that resembles the real dataset. The generator's goal is to produce data that is indistinguishable from the real data by the discriminator.\\n\\n2. **Discriminator:** This network evaluates the authenticity of the data it receives. It decides whether a given instance of data is real (from the dataset) or fake (generated by the generator). The discriminator is essentially a binary classifier designed to correctly categorize input data as real or fake.\\n\\nThe training process of GANs is a competition between these two networks. The generator aims to produce increasingly realistic data to fool the discriminator, while the discriminator improves its ability to distinguish real data from fake data. This adversarial nature leads to a minimax optimization framework where:\\n\\n- The generator tries to minimize the probability of the discriminator distinguishing the real from the fake.\\n- The discriminator tries to maximize the probability of correctly identifying real and fake data.\\n\\nThe process can be mathematically described by the following value function \\\\(V(G, D)\\\\):\\n\\n\\\\[ \\\\min_G \\\\max_D V(D, G) = \\\\mathbb{E}_{x \\\\sim p_{data}}[\\\\log D(x)] + \\\\mathbb{E}_{z \\\\sim p_z}[\\\\log(1 - D(G(z)))] \\\\]\\n\\nHere, \\\\(p_{data}\\\\) is the data distribution, \\\\(p_z\\\\) is the input noise distribution, and \\\\(z\\\\) is the noise vector used by the generator.\\n\\nAlthough GANs are powerful, they are notoriously difficult to train due to issues like mode collapse, non-convergence, and gradient instability. However, advances such as Wasserstein GANs (WGANs), Conditional GANs (CGANs), and StyleGANs have improved the stability and quality of data generated by GANs.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 483, 'prompt_tokens': 35, 'total_tokens': 518, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None} id='run-1920032e-9386-46e3-bc9b-521634960b04-0' usage_metadata={'input_tokens': 35, 'output_tokens': 483, 'total_tokens': 518, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Tell me about Generative Adversarial Network.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a class of machine learning frameworks invented by Ian Goodfellow and his colleagues in 2014. GANs are particularly known for their capability to generate new, synthetic instances of data that can pass for real data. They are widely used in image generation, video generation, and voice synthesis, among other applications.\n",
    "\n",
    "A GAN consists of two primary components: the generator and the discriminator, which are neural networks that contest with each other in a zero-sum game framework.\n",
    "\n",
    "1. **Generator:** This network is responsible for creating new data instances. It starts from a random noise and tries to generate data that resembles the real dataset. The generator's goal is to produce data that is indistinguishable from the real data by the discriminator.\n",
    "\n",
    "2. **Discriminator:** This network evaluates the authenticity of the data it receives. It decides whether a given instance of data is real (from the dataset) or fake (generated by the generator). The discriminator is essentially a binary classifier designed to correctly categorize input data as real or fake.\n",
    "\n",
    "The training process of GANs is a competition between these two networks. The generator aims to produce increasingly realistic data to fool the discriminator, while the discriminator improves its ability to distinguish real data from fake data. This adversarial nature leads to a minimax optimization framework where:\n",
    "\n",
    "- The generator tries to minimize the probability of the discriminator distinguishing the real from the fake.\n",
    "- The discriminator tries to maximize the probability of correctly identifying real and fake data.\n",
    "\n",
    "The process can be mathematically described by the following value function \\(V(G, D)\\):\n",
    "\n",
    "\\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] \\]\n",
    "\n",
    "Here, \\(p_{data}\\) is the data distribution, \\(p_z\\) is the input noise distribution, and \\(z\\) is the noise vector used by the generator.\n",
    "\n",
    "Although GANs are powerful, they are notoriously difficult to train due to issues like mode collapse, non-convergence, and gradient instability. However, advances such as Wasserstein GANs (WGANs), Conditional GANs (CGANs), and StyleGANs have improved the stability and quality of data generated by GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Autoencoders (VAEs) are a type of generative model that combine principles from deep learning and Bayesian inference to generate new data similar to a given dataset. They are an advanced version of traditional autoencoders, and they were introduced by Kingma and Welling in 2013.\n",
      "\n",
      "### Key Components of VAEs:\n",
      "\n",
      "1. **Encoder Network**: The encoder takes an input and maps it to a distribution over a latent space. Unlike traditional autoencoders, which encode an input as a fixed point in latent space, VAEs map inputs to a distribution, usually a Gaussian. The encoder outputs the mean and the logarithm of the variance of this distribution, representing the parameters of the Gaussian.\n",
      "\n",
      "2. **Latent Space**: Instead of a deterministic encoding, VAEs introduce stochasticity by representing each input as a Gaussian distribution in the latent space. A sample is drawn from this distribution during training, introducing some variability. This stochastic choice is crucial for generating diverse outputs.\n",
      "\n",
      "3. **Decoder Network**: The decoder takes a latent variable sampled from the Gaussian distribution and reconstructs the input data. The decoder is typically a neural network that learns to map latent variables back to the original data distribution.\n",
      "\n",
      "4. **Loss Function**: The loss function for a VAE consists of two parts:\n",
      "   - **Reconstruction Loss**: Measures how well the decoder reconstructs the input from the latent representation. This is usually computed as mean squared error for continuous data or cross-entropy for binary data.\n",
      "   - **KL Divergence**: A regularization term that ensures the learned latent space distribution is close to a prior distribution, typically a standard normal distribution. This encourages smoothness in the latent space and allows for meaningful sampling.\n",
      "\n",
      "5. **Reparameterization Trick**: A technique used to backpropagate through the sampling process. It involves expressing the sampling operation in a way that allows gradients to flow through the network by introducing a deterministic node and a noise variable.\n",
      "\n",
      "### Advantages of VAEs:\n",
      "\n",
      "- **Generative Capability**: Can generate new, similar data samples by sampling latent variables from the learned latent space.\n",
      "- **Continuous Latent Space**: Provides smooth interpolations between data points in the latent space, which can be useful for tasks such as morphing one data point into another.\n",
      "- **Unsupervised Learning**: VAEs can be trained without labeled data, making them suitable for unsupervised learning scenarios.\n",
      "\n",
      "### Applications:\n",
      "\n",
      "- Image generation and inpainting\n",
      "- Semi-supervised learning\n",
      "- Data denoising\n",
      "- Anomaly detection\n",
      "\n",
      "VAEs are a powerful framework for unsupervised learning and have been widely used in many applications, especially in scenarios where high-dimensional data needs to be compressed into a lower-dimensional space without losing much information.\n"
     ]
    }
   ],
   "source": [
    "# Print out just the sting answer only\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response2 = chain.invoke({\"input\": \"Tell me about Variation auto encodersk.\"})\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"output\": \"Variational Autoencoders (VAEs) are a type of generative model that combine principles from deep learning and Bayesian inference to generate new data similar to a given dataset. They are an advanced version of traditional autoencoders, and they were introduced by Kingma and Welling in 2013.\\n\\n### Key Components of VAEs:\\n\\n1. **Encoder Network**: The encoder takes an input and maps it to a distribution over a latent space. Unlike traditional autoencoders, which encode an input as a fixed point in latent space, VAEs map inputs to a distribution, usually a Gaussian. The encoder outputs the mean and the logarithm of the variance of this distribution, representing the parameters of the Gaussian.\\n\\n2. **Latent Space**: Instead of a deterministic encoding, VAEs introduce stochasticity by representing each input as a Gaussian distribution in the latent space. A sample is drawn from this distribution during training, introducing some variability. This stochastic choice is crucial for generating diverse outputs.\\n\\n3. **Decoder Network**: The decoder takes a latent variable sampled from the Gaussian distribution and reconstructs the input data. The decoder is typically a neural network that learns to map latent variables back to the original data distribution.\\n\\n4. **Loss Function**: The loss function for a VAE consists of two parts:\\n   - **Reconstruction Loss**: Measures how well the decoder reconstructs the input from the latent representation. This is usually computed as mean squared error for continuous data or cross-entropy for binary data.\\n   - **KL Divergence**: A regularization term that ensures the learned latent space distribution is close to a prior distribution, typically a standard normal distribution. This encourages smoothness in the latent space and allows for meaningful sampling.\\n\\n5. **Reparameterization Trick**: A technique used to backpropagate through the sampling process. It involves expressing the sampling operation in a way that allows gradients to flow through the network by introducing a deterministic node and a noise variable.\\n\\n### Advantages of VAEs:\\n\\n- **Generative Capability**: Can generate new, similar data samples by sampling latent variables from the learned latent space.\\n- **Continuous Latent Space**: Provides smooth interpolations between data points in the latent space, which can be useful for tasks such as morphing one data point into another.\\n- **Unsupervised Learning**: VAEs can be trained without labeled data, making them suitable for unsupervised learning scenarios.\\n\\n### Applications:\\n\\n- Image generation and inpainting\\n- Semi-supervised learning\\n- Data denoising\\n- Anomaly detection\\n\\nVAEs are a powerful framework for unsupervised learning and have been widely used in many applications, especially in scenarios where high-dimensional data needs to be compressed into a lower-dimensional space without losing much information.\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
